{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Adversarial Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright notice\n",
    "\n",
    "This version (c) 2021 Fabian Offert, [MIT License](LICENSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup\n",
    "\n",
    "Run the below commands only if you imported this notebook into Google Colab! Also **go to Runtime/Change runtime type and pick \"GPU\" as the hardware accelerator!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf adversarial # In case this is re-run\n",
    "!git clone https://github.com/zentralwerkstatt/adversarial\n",
    "!cp ./adversarial/erika_299x299.jpg ./\n",
    "!cp ./adversarial/giant_panda_299x299.jpg ./\n",
    "!cp ./adversarial/*synset_words.txt ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi # Check what kind of GPU we got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We are using PyTorch as our deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter, median_filter\n",
    "from skimage.restoration import denoise_bilateral, denoise_tv_chambolle\n",
    "import PIL.Image, PIL.ImageChops\n",
    "\n",
    "import os\n",
    "import random\n",
    "from io import BytesIO\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model to attack\n",
    "\n",
    "We are using the (very common) InceptionV3 architecture, pre-trained on ImageNet for most tasks, but also load VGG16 and VGG19 to test the \"universality\" of adversarial attacks (optional). **Colab users note: this may take a while, as the pre-trained weights have to be loaded in the background!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\") # Use GPU if available\n",
    "f1 = tv.models.inception_v3(pretrained=True).to(device)\n",
    "f2 = tv.models.vgg16(pretrained=True).to(device)\n",
    "f3 = tv.models.vgg19(pretrained=True).to(device)\n",
    "# Test mode: we do not want to train the model (i.e. change its weights) at any point\n",
    "f1.eval()\n",
    "f2.eval()\n",
    "f3.eval()\n",
    "model_names = {'f1':'Inception V3', 'f2':'VGG16', 'f3': 'VGG19'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Among other things, these helper functions allow us to convert between PyTorch tensors, NumPy arrays, and PIL images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an image within a Jupyter environment\n",
    "# Can do PyTorch tensors, NumPy arrays, and PIL images\n",
    "def show_img(img, title='', fmt='jpeg'):\n",
    "    if type(img) is np.ndarray:\n",
    "        img = PIL.Image.fromarray(img)\n",
    "    elif type(img) is t.Tensor:\n",
    "        img = deprocess(img)\n",
    "    out = BytesIO()\n",
    "    if title: print(title)\n",
    "    img.save(out, fmt)\n",
    "    display.display(display.Image(data=out.getvalue()))\n",
    "\n",
    "# PyTorch is channels first, this happens here!\n",
    "preprocess = tv.transforms.Compose([tv.transforms.ToTensor()])\n",
    "    \n",
    "# Reverse of preprocess, PyTorch tensor to PIL image\n",
    "def deprocess(tensor):\n",
    "    # Clone tensor first, otherwise we are NOT making a copy by using .cpu()!\n",
    "    img = t.clone(tensor)\n",
    "    img = img.cpu().data.numpy().squeeze() # Get rid of batch dimension\n",
    "    img = img.transpose((1, 2, 0)) # Channels first to channels last\n",
    "    \n",
    "    # We are not using ImageNet images as input\n",
    "    # mean = np.array([0.485, 0.456, 0.406]) \n",
    "    # std = np.array([0.229, 0.224, 0.225]) \n",
    "    # img = std * img + mean\n",
    "\n",
    "    # No clipping, adversarial regulation should take care of this\n",
    "    # img = np.clip(img, 0, 1)\n",
    "    \n",
    "    # 0./1. range to 0./255. range\n",
    "    img *= 255\n",
    "    \n",
    "    img = img.astype(np.uint8)\n",
    "    img = PIL.Image.fromarray(img)\n",
    "    return img\n",
    "\n",
    "# Return a gray square PIL image\n",
    "def gray_square(size):\n",
    "    # Gray square, -1./1. range\n",
    "    img = np.random.normal(0, 0.01, (size, size, 3)) \n",
    "    \n",
    "    # -1./1. range to 0./255. range\n",
    "    img /= 2.\n",
    "    img += 0.5\n",
    "    img *= 255.\n",
    "\n",
    "    img = img.astype(np.uint8)\n",
    "    img = PIL.Image.fromarray(img)\n",
    "    return img\n",
    "\n",
    "# Load ImageNet classes\n",
    "with open('synset_words.txt') as synset_words_file:\n",
    "    synset_words = synset_words_file.readlines()\n",
    "for i, line in enumerate(synset_words):\n",
    "    synset_words[i] = line.replace(' ', '_').replace(',', '_').lower().strip()\n",
    "\n",
    "# Classify an image with the target model \n",
    "# Can do PyTorch tensors and PIL images\n",
    "def predict(img, model):\n",
    "    if type(img) is t.Tensor:\n",
    "        preds = model(img.to(device))\n",
    "    else:\n",
    "        preds = model(preprocess(img).unsqueeze(0).to(device))\n",
    "    preds_softmax_np = F.softmax(preds, dim=1).cpu().data.numpy()\n",
    "    # Returns class no., class name, and prediction confidence\n",
    "    return preds_softmax_np.argmax(), synset_words[preds_softmax_np.argmax()], preds_softmax_np.max()\n",
    "\n",
    "# \"Rolling\" list: whenever an item is added, the first item is discarded\n",
    "def destructive_append(l,i):\n",
    "    l=l[1:]\n",
    "    l.append(i)\n",
    "    return l\n",
    "\n",
    "# PyTorch and skimage use different channel ordering\n",
    "def pytorch_to_skimage(img):\n",
    "    # No batch dimension\n",
    "    img = img[0]\n",
    "    # Channels last\n",
    "    img = np.swapaxes(img, 0, 2)\n",
    "    return img\n",
    "    \n",
    "def skimage_to_pytorch(img):\n",
    "    # Channels first\n",
    "    img = np.swapaxes(img, 0, 2)\n",
    "    # Skimage uses double\n",
    "    img = img.astype(np.float32)\n",
    "    # No Batch dimension\n",
    "    img = np.expand_dims(img, 0)\n",
    "    return img\n",
    "\n",
    "# Filters for feature visualization\n",
    "def filter_median(npimg, params):\n",
    "    npimg = median_filter(npimg, size=(1, 1, params['fsize'], params['fsize']))  \n",
    "    return npimg\n",
    "\n",
    "def filter_bilateral(npimg, params):\n",
    "    npimg = pytorch_to_skimage(npimg)\n",
    "    npimg = denoise_bilateral(npimg, sigma_color=0.05, sigma_spatial=15, multichannel=True)\n",
    "    npimg = skimage_to_pytorch(npimg)\n",
    "    return npimg\n",
    "\n",
    "def filter_TV(npimg, params):\n",
    "    npimg = pytorch_to_skimage(npimg)\n",
    "    npimg = denoise_tv_chambolle(npimg, weight=0.1, multichannel=True)\n",
    "    npimg = skimage_to_pytorch(npimg)\n",
    "    return npimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Gradient Sign Method (FGSM)\n",
    "\n",
    "From: Goodfellow, I.J., Shlens, J., Szegedy, C., 2014. [Explaining and harnessing adversarial examples](https://arxiv.org/abs/1412.6572). arXiv preprint arXiv:1412.6572."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm(img, neuron, model):\n",
    "    \n",
    "    η = 0.007 # Pertuberation amount\n",
    "    \n",
    "    # Preprocess input image and put on GPU\n",
    "    input = preprocess(img).unsqueeze(0).to(device).requires_grad_()\n",
    "    \n",
    "    # Reset gradients\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    x = model(input)\n",
    "    \n",
    "    # Use true label as optimum\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    # nn.CrossEntropyLoss() counter-intuitively does NOT take a one-hot vector as target!\n",
    "    label = t.tensor([neuron], dtype=t.long).to(device)\n",
    "    cost = loss(x, label)\n",
    "    cost.backward()\n",
    "    \n",
    "    attack_img = input + η*input.grad.sign()\n",
    "    attack_img = t.clamp(attack_img, 0.0, 1.0)\n",
    "    \n",
    "    return attack_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation over Transformation Method (ETO) / Adversarial Patch\n",
    "\n",
    "From: Brown, T.B., Mané, D., Roy, A., Abadi, M., Gilmer, J., 2017. [Adversarial patch](https://arxiv.org/abs/1712.09665). arXiv preprint arXiv:1712.09665, Athalye, A., Engstrom, L., Ilyas, A. and Kwok, K., 2017. [Synthesizing robust adversarial examples](https://arxiv.org/abs/1707.07397). arXiv preprint arXiv:1707.07397.\n",
    "\n",
    "Note: Only partially implemented (patch location) at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eto(img, neuron, model):\n",
    "        \n",
    "    LR = 0.4 # Yosinski learning rate\n",
    "    MIN_CONFIDENCE = 0.99 # Minimum prediction confidence to stop optimization\n",
    "    L2 = 1e-4 # Yosinski weight decay\n",
    "    \n",
    "    bg = PIL.Image.open('erika_299x299.jpg') # Fixed background for now\n",
    "    assert bg.width == bg.height # Assert background is square\n",
    "    assert img.width == img.height # Assert image to be optimized is square\n",
    "    assert img.width < bg.width # Assert image to be optimized is smaller than background\n",
    "\n",
    "    input = preprocess(img).unsqueeze(0).to(device).requires_grad_()\n",
    "    npbg = preprocess(bg).unsqueeze(0).data.cpu().numpy() # To tensor and back so we don not have to deal with channels etc.\n",
    "    optimizer = t.optim.SGD([input], lr=LR, weight_decay=L2)\n",
    "    \n",
    "    max_shift = npbg.shape[2]-input.shape[2]\n",
    "    \n",
    "    # We want to keep a running average, as the patch location is constantly changing\n",
    "    mem_confidence = 100\n",
    "    acc_confidence = [0.0 for i in range(mem_confidence)]\n",
    "    avg_confidence = 0.0\n",
    "    confidence = 0.0\n",
    "    \n",
    "    i = 0  \n",
    "    while avg_confidence < MIN_CONFIDENCE:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # TO DO: load random background image\n",
    "        # TO DO: scaling and rotation\n",
    "        npimg = input.data.cpu().numpy()\n",
    "        x_shift = np.random.randint(max_shift)\n",
    "        y_shift = np.random.randint(max_shift)\n",
    "        npcombined = npbg.copy()\n",
    "        npcombined[:,:,y_shift:y_shift+img.height,x_shift:x_shift+img.width] = npimg\n",
    "        input.data = t.from_numpy(npcombined).to(device)\n",
    "        \n",
    "        x = model(input)\n",
    "        loss = -x[:,neuron] # -x as the optimizer wants to minimize loss and we want to maximize class probability\n",
    "    \n",
    "        preds_softmax_np = F.softmax(x, dim=1).cpu().data.numpy()\n",
    "        confidence = preds_softmax_np[:,neuron]\n",
    "        \n",
    "        acc_confidence = destructive_append(acc_confidence, confidence)\n",
    "        avg_confidence = sum(acc_confidence)/mem_confidence\n",
    "            \n",
    "        i+=1\n",
    "        \n",
    "        if i%50 == 0: \n",
    "            print(f'Iterations: {i}, loss: {loss.item()}, pred.: {synset_words[preds_softmax_np.argmax()]}, avg. conf.: {avg_confidence}')\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        npcombined = input.data.cpu().numpy()\n",
    "        npimg = npcombined[:,:,y_shift:y_shift+img.height,x_shift:x_shift+img.width]\n",
    "        input.data = t.from_numpy(npimg).to(device)\n",
    "    \n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Least-Likely Class Method (ILLC)\n",
    "\n",
    "From: Kurakin, A., Goodfellow, I. and Bengio, S., 2016. [Adversarial examples in the physical world](https://arxiv.org/abs/1607.02533). arXiv preprint arXiv:1607.02533."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def illc(img, neuron, model):\n",
    "    \n",
    "    LR = 0.01 # Learning rate\n",
    "    η = 0.01 # Max pertuberation amount\n",
    "    MIN_CONFIDENCE = 0.99 # Minimum prediction confidence to stop optimization\n",
    "\n",
    "    input = preprocess(img).unsqueeze(0).to(device).requires_grad_()\n",
    "    original = input.data.cpu().numpy()\n",
    "    optimizer = t.optim.SGD([input], lr=LR)\n",
    "    \n",
    "    i = 0\n",
    "    confidence = 0.0\n",
    "    \n",
    "    while confidence < MIN_CONFIDENCE:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x = model(input)\n",
    "        loss = -x[:,neuron] # -x as the optimizer wants to minimize loss and we want to maximize class probability\n",
    "        \n",
    "        preds_softmax_np = F.softmax(x, dim=1).cpu().data.numpy()\n",
    "        confidence = preds_softmax_np[:,neuron]\n",
    "\n",
    "        i+=1\n",
    "\n",
    "        if i%50 == 0: \n",
    "            print(f'Iterations: {i}, loss: {loss.item()}, pred.: {synset_words[preds_softmax_np.argmax()]}, conf.: {confidence}')\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Regular and adversarial clipping on the CPU (don't mess with GPU tensors in place!)\n",
    "        img = input.data.cpu().numpy()\n",
    "        \n",
    "        clipped = np.where(img > original + η, original + η, img)\n",
    "        clipped = np.where(clipped < original - η, original - η, clipped)\n",
    "        # We could also use t.clamp() but as we are manipulating CPU representations anyway...\n",
    "        clipped = np.where(clipped > 1.0, 1.0, clipped)\n",
    "        clipped = np.where(clipped < 0.0, 0.0, clipped)\n",
    "        \n",
    "        input.data = t.from_numpy(clipped).to(device)\n",
    "    \n",
    "    return input"
   ]
  },
  {
   "source": [
    "### Generalized Gradient Ascent with Perceptual Filtering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ascent(img, neuron, model):\n",
    "\n",
    "    ITERATIONS = 2000\n",
    "    # FILTERS = [{'function':filter_median, 'frequency':4, 'params':{'fsize':5}}] # Good parameters\n",
    "    FILTERS = [{'function':filter_TV, 'frequency':20, 'params':{}}] # Good parameters\n",
    "    JITTER = 32\n",
    "    LR = 0.4\n",
    "    L2 = 1e-4 # Yosinski weight decay\n",
    "            \n",
    "    input = preprocess(img).unsqueeze(0).to(device).requires_grad_()\n",
    "    optimizer = t.optim.SGD([input], lr=LR, weight_decay=L2)\n",
    "    \n",
    "    for i in range(ITERATIONS):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Centers the object in the image\n",
    "        if JITTER:\n",
    "            npimg = input.data.cpu().numpy() # To CPU and numpy\n",
    "            ox, oy = np.random.randint(-JITTER, JITTER+1, 2)\n",
    "            npimg = np.roll(np.roll(npimg, ox, -1), oy, -2) # Jitter\n",
    "            input.data = t.from_numpy(npimg).to(device)\n",
    "\n",
    "        x = model(input)\n",
    "        loss = -x[:,neuron]\n",
    "\n",
    "        preds_softmax_np = F.softmax(x, dim=1).cpu().data.numpy()\n",
    "        confidence = preds_softmax_np[:,neuron]\n",
    "                    \n",
    "        if i%50 == 0: \n",
    "            print(f'Iterations: {i}, loss: {loss.item()}, pred.: {synset_words[preds_softmax_np.argmax()]}, conf.: {confidence}')\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Centers the object in the image\n",
    "        if JITTER:\n",
    "            npimg = input.data.cpu().numpy() # To CPU and numpy\n",
    "            npimg = np.roll(np.roll(npimg, -ox, -1), -oy, -2) # Jitter\n",
    "            input.data = t.from_numpy(npimg).to(device)\n",
    "            \n",
    "        # Stochastic clipping\n",
    "        input.data[input.data > 1] = np.random.uniform(0, 1)\n",
    "        input.data[input.data < 0] = np.random.uniform(0, 1)\n",
    "        \n",
    "        # Filtering\n",
    "        for filter_ in FILTERS:\n",
    "            if i != ITERATIONS - 1: # No regularization on last iteration for good quality output\n",
    "                if i % filter_['frequency'] == 0:\n",
    "                    npimg = input.data.cpu().numpy() # To CPU and numpy\n",
    "                    npimg = filter_['function'](npimg, filter_['params'])\n",
    "                    input.data = t.from_numpy(npimg).to(device)\n",
    "        # Verbose\n",
    "        if i%50==0:\n",
    "          show_img((input))\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erika = PIL.Image.open('erika_299x299.jpg')\n",
    "giant_panda = PIL.Image.open('giant_panda_299x299.jpg')\n",
    "show_img(erika)\n",
    "show_img(giant_panda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular image and predictions\n",
    "show_img(giant_panda)\n",
    "print(model_names['f1'], predict(giant_panda, f1))\n",
    "\n",
    "# Attack\n",
    "img = fgsm(giant_panda, giant_panda_id, f1)\n",
    "\n",
    "# Adversarial image and predictions\n",
    "show_img(img)\n",
    "print(model_names['f1'], predict(img, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ILLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular image\n",
    "show_img(erika)\n",
    "\n",
    "# Attack\n",
    "img = illc(erika, 1, f1) # 1 = Goldfish\n",
    "\n",
    "# Adversarial image and prediction\n",
    "show_img(img)\n",
    "print(model_names['f1'], predict(img, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amplify adversarial pattern\n",
    "diff = PIL.ImageChops.difference(deprocess(img), erika)\n",
    "show_img(diff)\n",
    "show_img(np.array(diff)*50)"
   ]
  },
  {
   "source": [
    "### General gradient ascent for feature visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = gray_square(299)\n",
    "img = gradient_ascent(noise, 1, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = eto(gray_square(50), 1, f1) # 1 = Goldfish\n",
    "show_img(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = PIL.Image.open('erika_299x299.jpg')\n",
    "npbg = preprocess(bg).unsqueeze(0).data.cpu().numpy()\n",
    "npimg = img.data.cpu().numpy()\n",
    "max_shift = npbg.shape[2]-img.shape[2]\n",
    "\n",
    "for i in range(1):\n",
    "    x_shift = np.random.randint(max_shift)\n",
    "    y_shift = np.random.randint(max_shift)\n",
    "    npcombined = npbg.copy()\n",
    "    npcombined[:,:,y_shift:y_shift+img.shape[2],x_shift:x_shift+img.shape[2]] = npimg\n",
    "    show_img(t.from_numpy(npcombined))\n",
    "    print(model_names['f1'], predict(t.from_numpy(npcombined), f1))\n",
    "    # print(model_names['f2'], predict(t.from_numpy(npcombined), f2))\n",
    "    # print(model_names['f3'], predict(t.from_numpy(npcombined), f3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Athalye, A., Engstrom, L., Ilyas, A., & Kwok, K. (2017). [Synthesizing robust adversarial examples](https://arxiv.org/abs/1707.07397). arXiv preprint arXiv:1707.07397.\n",
    "\n",
    "Brown, T.B., Mané, D., Roy, A., Abadi, M., & Gilmer, J. (2017). [Adversarial patch](https://arxiv.org/abs/1712.09665). arXiv preprint arXiv:1712.09665\n",
    "\n",
    "Carlini, N. & Wagner, D. (2017). [Adversarial examples are not easily detected: Bypassing ten detection methods](https://dl.acm.org/doi/pdf/10.1145/3128572.3140444). Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security\n",
    "\n",
    "Goodfellow, I.J., Shlens, J., & Szegedy, C. (2014). [Explaining and harnessing adversarial examples](https://arxiv.org/abs/1412.6572). arXiv preprint arXiv:1412.6572.\n",
    "\n",
    "Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., & Madry, A. (2019). [Adversarial examples are not bugs, they are features](https://arxiv.org/pdf/1905.02175.pdf). arXiv preprint arXiv:1905.02175.\n",
    "\n",
    "Kurakin, A., Goodfellow, I., & Bengio, S. (2016). [Adversarial examples in the physical world](https://arxiv.org/abs/1607.02533). arXiv preprint arXiv:1607.02533.\n",
    "\n",
    "Moosavi-Dezfooli, S. M., Fawzi, A., Fawzi, O., & Frossard, P. (2017). [Universal adversarial perturbations](https://openaccess.thecvf.com/content_cvpr_2017/papers/Moosavi-Dezfooli_Universal_Adversarial_Perturbations_CVPR_2017_paper.pdf). In Proceedings of the CVPR (pp. 1765-1773).\n",
    "\n",
    "Nguyen, A., Yosinski, J., Clune, J. (2015). [Deep neural networks are easily fooled: High confidence predictions for unrecognizable images](https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Nguyen_Deep_Neural_Networks_2015_CVPR_paper.html). In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 427–436.\n",
    "\n",
    "Ruiz, N., Bargal, S. A., & Sclaroff, S. (2020). [Disrupting deepfakes: Adversarial attacks against conditional image translation networks and facial manipulation systems](https://arxiv.org/pdf/2003.01279.pdf). arXiv preprint arXiv:2003.01279.\n",
    "\n",
    "Salman, H., Ilyas, A., Engstrom, L., Vemprala, S., Madry, A., & Kapoor, A. (2020). [Unadversarial Examples: Designing Objects for Robust Vision](https://arxiv.org/pdf/2012.12235.pdf). arXiv preprint arXiv:2012.12235.\n",
    "\n",
    "Su, J., Vargas, D.V., & Sakurai, K. (2019). [One pixel attack for fooling deep neural networks](https://ieeexplore.ieee.org/abstract/document/8601309). IEEE Transactions on Evolutionary Computation, 23(5), pp.828-841.\n",
    "\n",
    "Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. & Fergus, R. (2013). [Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199). arXiv preprint arXiv:1312.6199."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('lit-ada': conda)",
   "metadata": {
    "interpreter": {
     "hash": "cbba552011df5ccc8bc09bf1b64de4d563508c57d244dcff33d43b9b5d98878d"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}